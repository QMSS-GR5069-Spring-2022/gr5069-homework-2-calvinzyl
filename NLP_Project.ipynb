{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ***NLP Machine Learning Model Project*** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This project is under the mentorship by Dr. Xu Wang, currently the software engineer in Google Dublin office. The project uses the Consumer Complaint Database (n=2,418,392, k=18) of credit products from Consumer Financial Protection Bureau (CFPB).\n",
        "\n",
        "The project workflow takes reference from the following examples/tutorials:\n",
        "#\n",
        "https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/\n",
        "#\n",
        "https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
        "#\n",
        "https://github.com/kapadias/mediumposts/blob/master/natural_language_processing/topic_modeling/notebooks/Introduction%20to%20Topic%20Modeling.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIjufAPUPw7U",
        "outputId": "2421032f-58f1-416d-e65b-0ed382110706"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e26fvHxEQUmw"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster import hierarchy\n",
        "from math import ceil, floor\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plt.style.use('seaborn-white')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYQ2MdUdq08J",
        "outputId": "a03d97c3-959d-43c2-f5a0-2c6a57559168"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/yanlinzhang/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer, PorterStemmer\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m3CbE4tvm0cB"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from textblob import TextBlob\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bkhle31cQXVp"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DddH9grWQZVi"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KZOu8VyzQbHS"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "znBdicAZmlkV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "slJ_aWhkWeW9"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/Users/yanlinzhang/ProjectExample/complaints 2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "YrsSuTxJRd_l",
        "outputId": "eeb0bc98-192c-4090-a2d5-c0a9ad6aa056"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date received</th>\n",
              "      <th>Product</th>\n",
              "      <th>Sub-product</th>\n",
              "      <th>Issue</th>\n",
              "      <th>Sub-issue</th>\n",
              "      <th>Consumer complaint narrative</th>\n",
              "      <th>Company public response</th>\n",
              "      <th>Company</th>\n",
              "      <th>State</th>\n",
              "      <th>ZIP code</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Consumer consent provided?</th>\n",
              "      <th>Submitted via</th>\n",
              "      <th>Date sent to company</th>\n",
              "      <th>Company response to consumer</th>\n",
              "      <th>Timely response?</th>\n",
              "      <th>Consumer disputed?</th>\n",
              "      <th>Complaint ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-06-13</td>\n",
              "      <td>Credit reporting, credit repair services, or o...</td>\n",
              "      <td>Credit reporting</td>\n",
              "      <td>Incorrect information on your report</td>\n",
              "      <td>Information belongs to someone else</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CAPITAL ONE FINANCIAL CORPORATION</td>\n",
              "      <td>PA</td>\n",
              "      <td>18640</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Consent not provided</td>\n",
              "      <td>Web</td>\n",
              "      <td>2019-06-13</td>\n",
              "      <td>Closed with explanation</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3274605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-11-01</td>\n",
              "      <td>Vehicle loan or lease</td>\n",
              "      <td>Loan</td>\n",
              "      <td>Struggling to pay your loan</td>\n",
              "      <td>Denied request to lower payments</td>\n",
              "      <td>I contacted Ally on Friday XX/XX/XXXX after fa...</td>\n",
              "      <td>Company has responded to the consumer and the ...</td>\n",
              "      <td>ALLY FINANCIAL INC.</td>\n",
              "      <td>NJ</td>\n",
              "      <td>08854</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Consent provided</td>\n",
              "      <td>Web</td>\n",
              "      <td>2019-11-01</td>\n",
              "      <td>Closed with explanation</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3425257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-04-01</td>\n",
              "      <td>Credit reporting, credit repair services, or o...</td>\n",
              "      <td>Credit reporting</td>\n",
              "      <td>Incorrect information on your report</td>\n",
              "      <td>Account status incorrect</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Company has responded to the consumer and the ...</td>\n",
              "      <td>TRANSUNION INTERMEDIATE HOLDINGS, INC.</td>\n",
              "      <td>PA</td>\n",
              "      <td>19067</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Consent not provided</td>\n",
              "      <td>Web</td>\n",
              "      <td>2019-04-01</td>\n",
              "      <td>Closed with explanation</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3198225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-07-09</td>\n",
              "      <td>Student loan</td>\n",
              "      <td>Federal student loan servicing</td>\n",
              "      <td>Dealing with your lender or servicer</td>\n",
              "      <td>Don't agree with the fees charged</td>\n",
              "      <td>I was contacted about student loan consolidati...</td>\n",
              "      <td>Company believes it acted appropriately as aut...</td>\n",
              "      <td>Equitable Acceptance Corp</td>\n",
              "      <td>TX</td>\n",
              "      <td>75039</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Consent provided</td>\n",
              "      <td>Web</td>\n",
              "      <td>2019-07-09</td>\n",
              "      <td>Closed with explanation</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3300773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-08-08</td>\n",
              "      <td>Mortgage</td>\n",
              "      <td>Conventional home mortgage</td>\n",
              "      <td>Trouble during payment process</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Company has responded to the consumer and the ...</td>\n",
              "      <td>FLAGSTAR BANK, FSB</td>\n",
              "      <td>ID</td>\n",
              "      <td>83706</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Referral</td>\n",
              "      <td>2019-08-15</td>\n",
              "      <td>Closed with explanation</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3342290</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Date received                                            Product  \\\n",
              "0    2019-06-13  Credit reporting, credit repair services, or o...   \n",
              "1    2019-11-01                              Vehicle loan or lease   \n",
              "2    2019-04-01  Credit reporting, credit repair services, or o...   \n",
              "3    2019-07-09                                       Student loan   \n",
              "4    2019-08-08                                           Mortgage   \n",
              "\n",
              "                      Sub-product                                 Issue  \\\n",
              "0                Credit reporting  Incorrect information on your report   \n",
              "1                            Loan           Struggling to pay your loan   \n",
              "2                Credit reporting  Incorrect information on your report   \n",
              "3  Federal student loan servicing  Dealing with your lender or servicer   \n",
              "4      Conventional home mortgage        Trouble during payment process   \n",
              "\n",
              "                             Sub-issue  \\\n",
              "0  Information belongs to someone else   \n",
              "1     Denied request to lower payments   \n",
              "2             Account status incorrect   \n",
              "3    Don't agree with the fees charged   \n",
              "4                                  NaN   \n",
              "\n",
              "                        Consumer complaint narrative  \\\n",
              "0                                                NaN   \n",
              "1  I contacted Ally on Friday XX/XX/XXXX after fa...   \n",
              "2                                                NaN   \n",
              "3  I was contacted about student loan consolidati...   \n",
              "4                                                NaN   \n",
              "\n",
              "                             Company public response  \\\n",
              "0                                                NaN   \n",
              "1  Company has responded to the consumer and the ...   \n",
              "2  Company has responded to the consumer and the ...   \n",
              "3  Company believes it acted appropriately as aut...   \n",
              "4  Company has responded to the consumer and the ...   \n",
              "\n",
              "                                  Company State ZIP code Tags  \\\n",
              "0       CAPITAL ONE FINANCIAL CORPORATION    PA    18640  NaN   \n",
              "1                     ALLY FINANCIAL INC.    NJ    08854  NaN   \n",
              "2  TRANSUNION INTERMEDIATE HOLDINGS, INC.    PA    19067  NaN   \n",
              "3               Equitable Acceptance Corp    TX    75039  NaN   \n",
              "4                      FLAGSTAR BANK, FSB    ID    83706  NaN   \n",
              "\n",
              "  Consumer consent provided? Submitted via Date sent to company  \\\n",
              "0       Consent not provided           Web           2019-06-13   \n",
              "1           Consent provided           Web           2019-11-01   \n",
              "2       Consent not provided           Web           2019-04-01   \n",
              "3           Consent provided           Web           2019-07-09   \n",
              "4                        NaN      Referral           2019-08-15   \n",
              "\n",
              "  Company response to consumer Timely response? Consumer disputed?  \\\n",
              "0      Closed with explanation              Yes                NaN   \n",
              "1      Closed with explanation              Yes                NaN   \n",
              "2      Closed with explanation              Yes                NaN   \n",
              "3      Closed with explanation              Yes                NaN   \n",
              "4      Closed with explanation              Yes                NaN   \n",
              "\n",
              "   Complaint ID  \n",
              "0       3274605  \n",
              "1       3425257  \n",
              "2       3198225  \n",
              "3       3300773  \n",
              "4       3342290  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Text Preprocessing ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Data Undersampling ###\n",
        "\n",
        "Three raw variables will be (transformed) used in the modeling later: \"Issue\", \"Sub-issue\", \"Consumer Complaint Narrative\". I will explain the supervised learning model in later sections, but since the dependent variable will be the sentiment polarity score of \"Consumer Complaint Narrative\" and the independent variable X will be a sparse matrix stacking the tf-idf vectorized matrices of all three variables, I chose to simply undersample the imbalanced data, which is just to drop 1,572,930 rows from \"Issue\" and \"Sub-issue\" that both miss values from \"Consumer Complaint Narrative\", for the purpose of this project. An oversampling alternative could be employed in the future if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Date received                         0\n",
              "Product                               0\n",
              "Sub-product                      235164\n",
              "Issue                                 0\n",
              "Sub-issue                        642720\n",
              "Consumer complaint narrative    1574930\n",
              "Company public response         1429558\n",
              "Company                               0\n",
              "State                             38596\n",
              "ZIP code                          38809\n",
              "Tags                            2126380\n",
              "Consumer consent provided?       736491\n",
              "Submitted via                         0\n",
              "Date sent to company                  0\n",
              "Company response to consumer          3\n",
              "Timely response?                      0\n",
              "Consumer disputed?              1649933\n",
              "Complaint ID                          0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check missing values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new dataframe with only variables that will be used and drop rows with missing values from \"Consumer complaint narrative\"\n",
        "IV = df[['Issue','Sub-issue','Consumer complaint narrative']]\n",
        "IV.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Punctuation Removal ###\n",
        "\n",
        "The very first step here to preprocess our main text column is to remove the punctuation using the popular string.punctuation function, where the dictionary contains the following punctuations: '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'. Any punctuations covered by the very dictionary will be removed from the texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to remove punctuations included in the string.puntuation dictionary\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree\n",
        "\n",
        "# Store the puncutation-removed documents into a new column\n",
        "IV['complaint_nopunc']=IV['Consumer complaint narrative'].apply(lambda x:remove_punctuation(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Lowercasing ###\n",
        "\n",
        "Also very straightforwardly, the complaint narrative texts are then converted into the lowercases. This is not necessarily required, but our project focuses more on the actual content of complaints rather than primarily emotions (though important as well)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store the lowercased documents into a new column\n",
        "IV['complaint_lower']=IV['complaint_nopunc'].apply(lambda x: x.lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Stopwords Removal ###\n",
        "\n",
        "This step is to remove the stopwords, usually commonly used words that do not add much value to the analysis. Similar to most other text analysis, the popular NLTK stopword dictionary is used here, including, for example, personal pronouns like \"i\", \"you\", \"we\" and their inflections.\n",
        "\n",
        "Note that I extended the stopword list by adding x's, which are used to censor private or sensitive information from clients, like addresses. Likewise, for the purpose of our analysis, those x's carry less or no meaning.\n",
        "\n",
        "The table below is a comparison of before and after stopword removal of three randomly chosen complaint texts. The difference is pretty obvious."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call the NLTK stopwords dictionary and append a few x's that are specifically applicable to our complaint documents\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['x','xx','xxx','xxxx','xxxxx','xxxxxx','xxxxxxx','xxxxxxxx'])\n",
        "\n",
        "# Store the stopword-removed documents into a new column\n",
        "IV['complaint_nosw'] = IV['complaint_lower'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pull up a few documents before and after stopword removal\n",
        "compare = IV[['complaint_lower','complaint_nosw']].head(3)\n",
        "compare.style.set_properties(**{'width':'500px'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IV['complaint_lemmatized'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 Tokenization ###\n",
        "\n",
        "Tokenization is to split the text into smaller units, words in this case, which is particularly helpful in, for example, computing relative weight of individual words or modeling LDA. The word tokenization package from NLTK is used here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Store the tokenized documents, or tokens, into a new column\n",
        "IV['complaint_tokenized'] = IV['complaint_nosw'].apply(lambda x: word_tokenize(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.6 Lemmatization ###\n",
        "\n",
        "Lemmatization is sort of similar to stemming, reducing words to their root or base forms, but one disadvantage of stemming is that the root form of certain words is not formally an English word. On the contrary, lemmatization can ensure that the diminished words do not lose their meanings. It will pass words to pre-defined dictionary that stores the context of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet = WordNetLemmatizer()\n",
        "\n",
        "# Define a lemmatizer function using WordNetLemmatizer\n",
        "def lemmatizer(text):\n",
        "    lemm_text = [wordnet.lemmatize(word) for word in text]\n",
        "    return lemm_text\n",
        "\n",
        "# Store the lemmatized tokens into a new column\n",
        "IV['complaint_lemmatized']=IV['complaint_tokenized'].apply(lambda x: lemmatizer(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.7 WordCloud Visualization ###\n",
        "\n",
        "Wordcloud is a textual visualization technique where each word is picturized with regard to it's importance/relative weight. After we lowercases the documents and removed the stopwords and punctuations, we can visualize the text data in the form of words, where the importance of a word is explained by its frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "long_string = ','.join(list(IV['complaint_nosw'].astype(str).values))\n",
        "\n",
        "wordcloud = WordCloud(background_color = 'black', max_words = 1000,\n",
        "                      contour_width=5, contour_color='steelblue')\n",
        "\n",
        "wordcloud.generate(long_string)\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Topic Modeling ##\n",
        "\n",
        "Topic modeling on text data is very much like the clustering analysis with regard to numerical data. It's similar to fuzzy clustering where each data point can belong to more than one subgroup or cluster, while in topic modeling one particular document can be part of multiple topics. As a type of unsupervised learning in the context of text analysis, topic models are used to discover hidden topics or groups of topics based on the text data we have, which enables us to understand the documents better and lots of times to proceed with subsequent supervised modeling procedures.\n",
        "\n",
        "Latent Dirichlet Allocation (LDA), one of the most frequently used algorithms in topics modeling, is generative probabilistic model for collections of discrete data. LDA is a three-level hierarchical Bayesian model, where each item of a collection is modeled as a finite mixture attributable to one of the underlying set of latent topics. After pre-specifying the number of topics before deploying it, LDA returns the sorted words in each topic with respect to their probability score. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Prepare for LDA Modeling ###\n",
        "\n",
        "There are a few preparatory steps before deploying LDA. \"Id2word\" enables the mapping between normalized words and their integer ids. The \"doc2bow\" attribute is used to convert the document into the bag-of-words format, i.e., the 2-tuples (token_id, token_count), which in our case is exerted onto the lemmatized complaint narrative documents. Lastly, I specified the number of topics k to be five because it has one of the highest coherence scores, around 0.5. Tuning k is illustrated in section 2.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "\n",
        "data_words = IV['complaint_lemmatized']\n",
        "\n",
        "# Map the normalized words and their integer ids\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "texts = data_words\n",
        "# Create a corpus storing bow-converted lemmatized words \n",
        "corpus = [id2word.doc2bow(text) for text in texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Finding Optimal Number of Topics ###\n",
        "\n",
        "The way here to ascertain the optimal number of topics in LDA is to run different LDA models with different numbers of topics, k, and see which one returns the highest coherence score. Topic coherence is defined as the measure of score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. In here the \"c_v\" coherence measure is used, which, by definition, is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information and the cosine similarity. The higher the coherence score an LDA model returns, the more coherent, and usually the better the model is in clustering groups of topics.\n",
        "\n",
        "Similar to the elbow curve of choosing the optimal k in the context of k-means clustering, the elbow curve of LDA visualizes the trend of changing coherence score when the number of topics increases. It makes sense to pick the k with relatively higher coherence score (not necessarily the highest) but also marks an \"elbow\" turning point where the topic coherence curve starts to flatten out. If some keywords appear repetitively in multiple topics, then the LDA might allocate too many topics even if the coherence score is rather high. An optimal LDA model ideally should be both coherent and have distinct words in each topic it partitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coherence_score = []\n",
        "\n",
        "# Formulate a for loop iterating LDA & coherence modeling over a sequence of k from 1 to 20\n",
        "for k in range (1, 20):\n",
        "    lda_tune = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                 id2word=id2word,\n",
        "                                 num_topics=k)\n",
        "    coherence_tune = CoherenceModel(model=lda_tune, texts=data_words,\n",
        "                                    dictionary = id2word, coherence='c_v')\n",
        "    coherence_score.append(coherence_tune.get_coherence())\n",
        "\n",
        "# Visualize an Elbow Curve of coherence score against number of topics\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "plt.plot(range(1, 20), coherence_score, marker='o',color='black')\n",
        "plt.xlabel('Number of Topics')\n",
        "plt.ylabel('Coherence Score')\n",
        "plt.title('Elbow Curve') \n",
        "plt.grid(True)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the Elbow Curve above, a reasonable k would be 7, which means seven topics in the LDA model will return one of those higher coherence scores and will simultanously have more distinct words in each topic. Note that the elbow curve of LDA is display an increasing concave shape very roughly, which makes the choice of k harder and more subjective. K=7, if not the best, is at least a very acceptable choice right here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Building LDA Model ##\n",
        "\n",
        "With number of topics specified, we can proceed with the model construction. Again, id2word is the dictionary mapping tokens to the corresponding token id's, and corpus is the list storing tuples obtained from converting documents into the bag-of-words (BoW) format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Pre-specify the paramter k\n",
        "topics = 7\n",
        "\n",
        "# Deploy LDA model\n",
        "lda = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                 id2word=id2word,\n",
        "                                 num_topics=topics)\n",
        "\n",
        "pprint(lda.print_topics())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the LDA output, we can try to interpret some of the topics LDA provides. For example, Topic 1 is represented by '0.021*\"account\" + 0.016*\"bank\" + 0.012*\"card\" + 0.009*\"told\" + ''0.009*\"credit\" + 0.008*\"would\" + 0.008*\"time\" + 0.008*\"called\" + ''0.007*\"back\" + 0.007*\"check\"'. It means the top 10 keywords in this particular topic are \"account\", \"bank\", \"card\", and so on. Topic 1, therefore, is probably focusing on personal bank transactions. Other topics could also be inferred from the corresponding top 10 keywords.\n",
        "\n",
        "Although there are some of the common words appearing in multiple topics like \"credit\" and \"account\", they are intuitively frequent occurrences in coherent narrative regarding credit products. That is to say, the seven topics our LDA model offers are relatively unique and should represent different focuses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "print('\\nPerplexity: ', lda.log_perplexity(corpus))\n",
        "\n",
        "coherence_lda = CoherenceModel(model=lda, texts=data_words,\n",
        "                               dictionary=id2word, coherence='c_v')\n",
        "coherence = coherence_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perplexity score is the normalized inverse probability of the test set. Both perplexity and coherence scores are intrinsic evaluation metrics which evaluate the language model, like LDA, itself rather than employing the model in an actual task. If our LDA model assigns a high probability to the test set, it means the model is not perplexed by the words in the test set, which means the job done by the model is more understandable and coherent. Therefore, a perplexity of -7.08 indicates a good topic model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "visual = pyLDAvis.gensim_models.prepare(lda, corpus, id2word)\n",
        "visual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pyLDAvis is a very powerful tool to visualize and understand the topics, especially if combination of words from the LDA output is still confusing. The interactive dashboard has bubbles, blue bars, and red bars. Each bubble represents a topic, and the size of the bubble corresponds to the number of words in that topic. The farther away the bubbles are from each other, the more different those topics are. Blue bar represents the overall frequency of each word. Red bar, if a bubble is selected, represents the estimated number of times a given word was generated by a given topic.\n",
        "\n",
        "On the intertopic distance map, our seven topics are quite spread out overall, but certain topics are more similar than the others. Topic 3, 5, 6 have overalapping areas, sort of like a three-way Venn diagram, while Topic 1 and 2 are very similar both in terms of words themselves and number of words. Top 7 and 4, on the contrary, are farther away from those two \"bubble clusters\". For example, Topic 7 consists of words including \"consumer\", \"information\", \"agency\", \"call\", etc., that appear less frequently in other topics. Topic 4 has words like \"debt\", \"act\", \"claim\", \"law\", \"violation\", which might be focusing on some more serious accusations relevant to credit products.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sentiment Analysis ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 TextBlob Sentiment Polarity ###\n",
        "\n",
        "TextBlob is a Python library for processing textual data, and in here I'm using sentiment.polarity property which returns a document's polarity score within the rage [-1.0, 1.0], where -1.0 is very negative and 1.0 is very positive. Note that the original complaint narrative column is used here rather than the lemmatized complaint because it makes a lot more sense to capture the sentiment polarity of original documents with certain tone, linguistic subtlety, and coherent content being kept.\n",
        "\n",
        "The second cell is meant to label the complaints into three categories: \"Positive\", \"Negative\", and \"Neutral\", based on the polarity score. This is also a preliminary step for comparing with another library: VADER, and possibly supervised learning models as well where the \"sentiment1\" column could be the target of classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to compute TextBlob sentiment polarity scores correspond to each document\n",
        "def senti(x):\n",
        "    return TextBlob(x).sentiment.polarity\n",
        "\n",
        "# Store the TextBlob polarity scores into a new column    \n",
        "IV['score1']=IV['Consumer complaint narrative'].apply(lambda x:senti(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to classify documents into categories based on polarity scores\n",
        "def analysis(x):\n",
        "    if x < 0:\n",
        "        return 'Negative'\n",
        "    elif x == 0:\n",
        "        return 'Neutral'\n",
        "    else:\n",
        "        return 'Positive'\n",
        "\n",
        "# Store the labels into a new column\n",
        "IV['sentiment1']=IV['score1'].apply(lambda x: analysis(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 VADER Compound Score ###\n",
        "\n",
        "Valence Aware Dictionary and sEntiment Reaonser (VADER) is a lexicon and rule-based sentiment analysis tool, frequently used to analyze social media posts or tweets. The Sentiment Intensity Analyzer returns the intensity scores of negative, neutral, and positive components of a document that add up to one, as well as a compound score used to draw the overall sentiment, within the range of [-1.0, 1.0], which is comparable to the TextBlob polarity score. \n",
        "\n",
        "Similar procedure is repeated down here: compute the VADER compound scores corresponding to complaint narrative documents, and label the complaints into three categories: \"Positive\", \"Negative\", and \"Neutral\" based on the compound scores. Note that compounds scores higher or equal to 0.5 is considered positive, those lower or equal to -0.5 is considered negative, and any scores in-between correspond to neutral sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Define a function to compute VADER sentiment compound scores correspond to each document\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "def vader(x):\n",
        "    vs = analyzer.polarity_scores(x)\n",
        "    return vs['compound']\n",
        "\n",
        "# Store the VADER compound scores into a new column\n",
        "IV['score2'] = IV['Consumer complaint narrative'].apply(vader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to classify documents into categories based on compound scores\n",
        "def vader_analysis(compound):\n",
        "    if compound >= 0.5:\n",
        "        return \"Positive\"\n",
        "    elif compound <= -0.5:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "# Store the labels into a new column\n",
        "IV['sentiment2']=IV['score2'].apply(lambda compound: vader_analysis(compound))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Comparison of Two Methods ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group the polarity and compound scores by their labels (positive, neutral, or negative)\n",
        "tb_counts = IV['sentiment1'].value_counts().sort_index(ascending=False)\n",
        "vader_counts = IV['sentiment2'].value_counts().sort_index(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to add value labels covered in rectangular box at the center of the height\n",
        "# of each bar of bar chart\n",
        "def addlabels(x,y):\n",
        "    for i in range(len(x)):\n",
        "        plt.text(i, y[i]//2, y[i], ha = 'center',\n",
        "                 Bbox = dict(facecolor = 'white', alpha = 0.5))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    x = tb_counts.index\n",
        "    y = tb_counts.values\n",
        "\n",
        "    plt.figure(figsize=(17,6))\n",
        "    \n",
        "    # Create two subplots each visualizing the counts distribution of sentiment labels\n",
        "    # Set ylim to let both subplots be to the same scale\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"TextBlob - Distribution of Sentiment Categories\")\n",
        "    plt.bar(x, y, width=0.7, color='sienna')\n",
        "    addlabels(x, y)\n",
        "    plt.ylim(0, 320000)\n",
        "\n",
        "    x = vader_counts.index\n",
        "    y = vader_counts.values\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"VADER - Distribution of Sentiment Categories\")\n",
        "    plt.bar(x, y, width=0.7, color='sandybrown')\n",
        "    addlabels(x, y)\n",
        "    plt.ylim(0, 320000)\n",
        "\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Supervised Learning - Text Classification ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Vectorizing Textual Data ###\n",
        "\n",
        "TF-IDF (Term Frequency - Inverse Document Frequency) is a weighting scheme that evaluates how relevant a word is to document in a collection of documents. It is a popular statistical measure meant to convert text to numerical format (sparse matrix) for machine learning purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Sparse Matrix Shape: (670772, 124192)\n",
            "Number of Features: 408\n"
          ]
        }
      ],
      "source": [
        "tfidf_vect = TfidfVectorizer(analyzer='word')\n",
        "\n",
        "# Convert three collections of raw documents to three sparse matrices of TF-IDF features\n",
        "tfidf_com = tfidf_vect.fit_transform(IV['Consumer complaint narrative'])\n",
        "tfidf_issue = tfidf_vect.fit_transform(IV['Issue'])\n",
        "tfidf_sub = tfidf_vect.fit_transform(IV['Sub-issue'])\n",
        "\n",
        "# Stack three sparse matrices horizontally\n",
        "tfidfBIG=scipy.sparse.hstack((tfidf_com, tfidf_issue, tfidf_sub))\n",
        "print(\"TF-IDF Sparse Matrix Shape: {}\".format(tfidfBIG.shape))\n",
        "print(\"Number of Features: {}\".format(len(tfidf_vect.get_feature_names())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Dimensionality Reduction ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original shape:  (670772, 124192)\n",
            "transformed shape:  (670772, 500)\n",
            "explained variance ratio sum:\n",
            " 0.8365072811813636\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "svd = TruncatedSVD(n_components=500, n_iter=10, random_state=42)\n",
        "sam = svd.fit_transform(tfidfBIG)\n",
        "\n",
        "print(\"original shape: \", tfidfBIG.shape)\n",
        "print(\"transformed shape: \", sam.shape)\n",
        "print(\"explained variance ratio sum:\\n\", svd.explained_variance_ratio_.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = IV['sentiment2']\n",
        "X = tfidfBIG\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Logistic Regression ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piPaxmq5aQxx",
        "outputId": "99bcdc2d-604e-4775-8151-ca3f97a1634b"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "\n",
        "param_l2 = {'C': np.arange(1, 200, 1),\n",
        "            'penalty':['l2'],\n",
        "            'solver':['saga','newton-cg','liblinear']}\n",
        "\n",
        "kf = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
        "\n",
        "grid_l2 = RandomizedSearchCV(LogisticRegression(max_iter=1000), param_l2, cv=kf, verbose=2)\n",
        "grid_l2.fit(X_train, y_train)\n",
        "\n",
        "print(\"best mean cross-validation score: {:.3f}\".format(grid_l2.best_score_))\n",
        "print(\"best parameters: {}\".format(grid_l2.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set score: 0.69372\n",
            "Test set score: 0.69108\n",
            "Mean Cross Validation score (stratified k-fold): 0.69216\n"
          ]
        }
      ],
      "source": [
        "logit_best = LogisticRegression(C=18, penalty='l2',solver='saga').fit(X_train, y_train)\n",
        "\n",
        "print(\"Training set score: {:.5f}\".format(logit_best.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.5f}\".format(logit_best.score(X_test, y_test)))\n",
        "print(\"Mean Cross Validation score (stratified k-fold): {:.5f}\".format(np.mean(cross_val_score(logit_best, X_train, y_train, cv=kf))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "kf = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logit_best = LogisticRegression(C=18, penalty='l2',solver='saga').fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logit_best.score(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cross_val_score(logit_best, X_train, y_train, cv=kf, scoring='roc_auc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "roc_auc_score(y_train, logit_best.predict_proba(X_train), multi_class='ovr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f1_score(y_train, logit_best.predict(X_train), average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
        "\n",
        "kf = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm_best = SVC().fit(X_train, y_train)\n",
        "\n",
        "print(\"Training set score: {:.5f}\".format(svm_best.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.5f}\".format(svm_best.score(X_test, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "param_svc = {'C': [50, 10, 1.0, 0.1, 0.01],\n",
        "            'kernel':['poly','rbf','sigmoid'],\n",
        "            'gamma':['scale']}\n",
        "\n",
        "grid_svc = RandomizedSearchCV(SVC(), param_svc, cv=kf, verbose=2, n_jobs=-1)\n",
        "grid_svc.fit(X_train, y_train)\n",
        "\n",
        "print(\"best mean cross-validation score: {:.3f}\".format(grid_svc.best_score_))\n",
        "print(\"best parameters: {}\".format(grid_svc.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import naive_bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Negative values in data passed to MultinomialNB (input X)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m p \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormalizing\u001b[39m\u001b[38;5;124m'\u001b[39m,MaxAbsScaler()),(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultimonialNB\u001b[39m\u001b[38;5;124m'\u001b[39m,MultinomialNB())])\n\u001b[0;32m----> 2\u001b[0m naive1 \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py:394\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    393\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py:690\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    688\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[0;32m--> 690\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_alpha()\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_feature_log_prob(alpha)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py:863\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;124;03m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultinomialNB (input X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m safe_sparse_dot(Y\u001b[38;5;241m.\u001b[39mT, X)\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1249\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1246\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m whom)\n",
            "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
          ]
        }
      ],
      "source": [
        "p = Pipeline([('Normalizing',MaxAbsScaler()),('MultimonialNB',MultinomialNB())])\n",
        "naive1 = p.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_scaled = MaxAbsScaler().fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_scaled' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX_scaled\u001b[49m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_scaled' is not defined"
          ]
        }
      ],
      "source": [
        "X_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set score: 0.53787\n",
            "Test set score: 0.53383\n"
          ]
        }
      ],
      "source": [
        "naive = MultinomialNB().fit(X_train, y_train)\n",
        "\n",
        "print(\"Training set score: {:.5f}\".format(naive.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.5f}\".format(naive.score(X_test, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PowerTransformer\n\u001b[0;32m----> 3\u001b[0m params_NB \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvar_smoothing\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mlogspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)}\n\u001b[1;32m      5\u001b[0m gs_NB \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(estimator\u001b[38;5;241m=\u001b[39mMultinomialNB(),\n\u001b[1;32m      6\u001b[0m                      param_grid\u001b[38;5;241m=\u001b[39mparams_NB,\n\u001b[1;32m      7\u001b[0m                      cv\u001b[38;5;241m=\u001b[39mkf,\n\u001b[1;32m      8\u001b[0m                      verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      9\u001b[0m                      n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     10\u001b[0m                      scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m PowerTransformer()\u001b[38;5;241m.\u001b[39mfit_transform(X_train\u001b[38;5;241m.\u001b[39mtoarray())\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "params_NB = {\"var_smoothing\": np.logspace(0, -9, num=100)}\n",
        "\n",
        "gs_NB = RandomizedSearchCV(estimator=MultinomialNB(),\n",
        "                     param_grid=params_NB,\n",
        "                     cv=kf,\n",
        "                     verbose=1,\n",
        "                     n_jobs=-1,\n",
        "                     scoring='accuracy')\n",
        "\n",
        "X_scaled = PowerTransformer().fit_transform(X_train.toarray())\n",
        "\n",
        "gs_NB.fit(X_scaled, y_train)\n",
        "print(\"best mean cross-validation score: {:.3f}\".format(gs_NB.best_score_))\n",
        "print(\"best parameters: {}\".format(gs_NB.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Google PTA Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
